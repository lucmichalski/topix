{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "tokenizer_sp = SentencePieceBPETokenizer(unk_token='[UNK]')\n",
    "tokenizer_sp.add_tokens(['[SEP]','[CLS]', '[UNK]'])\n",
    "tokenizer_sp.add_special_tokens(['[NUMBER]','[DATE]'])\n",
    "tokenizer_sp.train(files=[\"./train2.clean.txt\", \"./test2.clean.txt\"], show_progress=True, vocab_size=30000)\n",
    "tokenizer_sp.save(directory='tokenizer_sp', name='hebrew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer_wp = BertWordPieceTokenizer(lowercase=True, vocab_file=None, add_special_tokens=True, unk_token='[UNK]', \n",
    "                                      sep_token='[SEP]', cls_token='[CLS]', clean_text=True, handle_chinese_chars=False, \n",
    "                                      strip_accents=True, wordpieces_prefix='##')\n",
    "tokenizer_wp.add_tokens(['[NUMBER]','[DATE]'])\n",
    "tokenizer_wp.train(files=[\"./train2.clean.txt\", \"./test2.clean.txt\"], show_progress=True, vocab_size=30000)\n",
    "tokenizer_wp.save(directory='tokenizer_wp', name='hebrew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/24/2020 13:09:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/24/2020 13:09:15 - INFO - transformers.tokenization_utils -   Model name './tokenizer_wp/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './tokenizer_wp/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/24/2020 13:09:15 - INFO - transformers.tokenization_utils -   Didn't find file ./tokenizer_wp/added_tokens.json. We won't load it.\n",
      "03/24/2020 13:09:15 - INFO - transformers.tokenization_utils -   Didn't find file ./tokenizer_wp/special_tokens_map.json. We won't load it.\n",
      "03/24/2020 13:09:15 - INFO - transformers.tokenization_utils -   Didn't find file ./tokenizer_wp/tokenizer_config.json. We won't load it.\n",
      "03/24/2020 13:09:15 - INFO - transformers.tokenization_utils -   loading file ./tokenizer_wp/vocab.txt\n",
      "03/24/2020 13:09:15 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/24/2020 13:09:15 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/24/2020 13:09:15 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/24/2020 13:09:15 - INFO - __main__ -   Training new model from scratch\n",
      "03/24/2020 13:09:27 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1000000000000, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='test2.clean.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=15.0, output_dir='hebrew_bert', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=5, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='./tokenizer_wp/', train_data_file='train2.clean.txt', warmup_steps=0, weight_decay=0.0)\n",
      "03/24/2020 13:09:27 - INFO - __main__ -   Creating features from dataset file at train2.clean.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"./transformers/examples/run_language_modeling.py\", line 799, in <module>\n",
      "    main()\n",
      "  File \"./transformers/examples/run_language_modeling.py\", line 744, in main\n",
      "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
      "  File \"./transformers/examples/run_language_modeling.py\", line 150, in load_and_cache_examples\n",
      "    return LineByLineTextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
      "  File \"./transformers/examples/run_language_modeling.py\", line 136, in __init__\n",
      "    lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.6/codecs.py\", line 321, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_language_modeling.py --model_type=bert --train_data_file=train2.clean.txt --eval_data_file=test2.clean.txt --output_dir=hebrew_bert2 --num_train_epochs=15 --save_total_limit=5 --save_steps=500 --tokenizer_name=./tokenizer_wp_lior/ --overwrite_output_dir --mlm --do_train --do_eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
